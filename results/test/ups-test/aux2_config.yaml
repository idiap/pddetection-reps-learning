# Auxiliary model/training configuration
# Copyright (c) 2021 Idiap Research Institute, http://www.idiap.ch/
# Written by Parvaneh Janbakhshi <parvaneh.janbakhshi@idiap.ch>

SelectedAuxiliaryNetwork: "MLP"
SelectedEncodedLayers: -1                        # list of indices of MLP layers or -1. If it is a list, it concatenates the output of layers with indices in the list
                                                 # otherwise (-1) last layer of MLP encoder is used.

MLP:                                             # Dropout > Linear layer > nonlinearity > Linear layer > nonlinearity ... > Linear layer > outputdim
  outputdim: 2                                   # number of classes--> output size
  hidden_units: [64, 64]                         # Number of layers and their hidden units before output. if is [] means 1 layer for connecting input and output
  nonlinearity: "leaky-relu"                     # The non-linear activation function "leaky-relu", "relu" or no nonlinearity ""
  dropout_prob: 0.2                              # The dropout probability after encoded layer (before MLP)

dataloader:
  online: False                                  # If True it computes feature-on-the-fly (online), otherwise uses saved features
  num_workers: 4                                 # torch Dataloader workers
  batch_size: 128                                # batch size
  sequence_length: 500                           # miliseconds (--> frames) segmenting audio as inputs for networks (both online and offline)
  data_path: 'preprocess/dummy_database/folds/'   # Source data path, 'in folder preprocess/Dataset_name/folds
  fs: 16000                                      # sampling frequency (needed for online feature extraction)

                                                 # Training options
runner:
 # the following commented options are dictated by the main upstream config file
  # Max_epoch: 100                               # total steps for training
  optimizer:
    # type: 'Adam'                               # optimizer type: ['Adam', 'SGD'].
    # lr: 1e-3                                   # Learning rate for downstream opt.
    # minlr: 1e-7                                # minimum (lower bound) lr after decreasing learning rate for early stopping
    auxlr_ratio: 1                               # ratio of learning rate for auxiliary training such that the learning rate: aux_lr = upslr_ratio * lr
    loss: "CE"                                   # loss: MSE or CE (Cross Entropy)

